# 可见性与缓存一致性

所谓的可见性，即是一个线程对共享变量的修改，另外一个线程能够立刻看到。单核时代，所有的线程都是直接操作单个 CPU 的数据，某个线程对缓存的写对另外一个线程来说一定是可见的；譬如下图中，如果线程 B 在线程 A 更新了变量值之后进行访问，那么获得的肯定是变量 V 的最新值。多核时代，每颗 CPU 都有自己的缓存，共享变量存储在主内存。运行在某个 CPU 中的线程将共享变量读取到自己的 CPU 缓存。在 CPU 缓存中，修改了共享对象的值，由于 CPU 并未将缓存中的数据刷回主内存，导致对共享变量的修改对于在另一个 CPU 中运行的线程而言是不可见的。这样每个线程都会拥有一份属于自己的共享变量的拷贝，分别存于各自对应的 CPU 缓存中。

![CPU、内存与变量](https://i.postimg.cc/HnKnNPmq/image.png)

# MESI 协议

高速缓存内部是一个拉链散列表，是不是很眼熟，是的，和 HashMap 的内部结构十分相似，高速缓存中分为很多桶，每个桶里用链表的结构连接了很多 cache entry，在每一个 cache entry 内部主要由三部分内容组成：

- tag：指向了这个缓存数据在主内存中的数据的地址
- cache line：存放多个变量数据
- flag：缓存行状态

![Cache Struct](https://s3.ax1x.com/2021/01/28/y9tmKH.png)

各个处理器在操作内存数据时，都会往总线发送消息，各个处理器还会不停的从总线嗅探消息，通过这个消息来保证各个处理器的协作。同时 MESI 中有以下两个操作：

- flush 操作：强制处理器在更新完数据后，将更新的数据（可能写缓冲器、寄存器中）刷到高速缓存或者主内存（不同的硬件实现 MESI 协议的方式不一样），同时向总线发出信息说明自己修改了某一数据
- refresh 操作：从总线嗅探到某一数据失效后，将该数据在自己的缓存中失效，然后从更新后的处理器高速缓存或主内存中加载数据到自己的高速缓存中

## CPU 读写流程

传统的 MESI 协议中有两个行为的执行成本比较大。一个是将某个 Cache Line 标记为 Invalid 状态，另一个是当某 Cache Line 当前状态为 Invalid 时写入新的数据。所以 CPU 通过 Store Buffer 和 Invalidate Queue 组件来降低这类操作的延时。如图：

![CPU 间高速缓存示意图](https://i.postimg.cc/8kRGLBr3/image.png)

当一个核心在 Invalid 状态进行写入时，首先会给其它 CPU 核发送 Invalid 消息，然后把当前写入的数据写入到 Store Buffer 中。然后异步在某个时刻真正的写入到 Cache Line 中。当前 CPU 核如果要读 Cache Line 中的数据，需要先扫描 Store Buffer 之后再读取 Cache Line（Store-Buffer Forwarding）。但是此时其它 CPU 核是看不到当前核的 Store Buffer 中的数据的，要等到 Store Buffer 中的数据被刷到了 Cache Line 之后才会触发失效操作。而当一个 CPU 核收到 Invalid 消息时，会把消息写入自身的 Invalidate Queue 中，随后异步将其设为 Invalid 状态。和 Store Buffer 不同的是，当前 CPU 核心使用 Cache 时并不扫描 Invalidate Queue 部分，所以可能会有极短时间的脏读问题。

接下来我们来说明在两个处理器情况下，其中一个处理器（处理器 0）要修改数据的整个过程。假定数据所在 cache line 在两个高速缓存中都处于 S(Shared)状态。

![CPU 处理过程](https://s3.ax1x.com/2021/01/28/y9tHQe.png)

- 处理器 0 发送 invalidate 消息到总线；
- 处理器 1 在总线上进行嗅探，嗅探到 invalidate 消息后，通过地址解析定位到对应的 cache line，发现此时 cache line 的状态为 S，则将 cache line 的状态改为 I，同时返回 invalidate ack 消息到总线；
- 处理器 0 在总线在嗅探到所有（例子中只有处理器 1）的 invalidate ack 后，将要修改的 cache line 状态置为 E(Exclusive)，表示要进行独占修改，修改完以后将 cache line 状态置为 M(Modified)，同时可能将数据刷回主内存。

在这个过程中，如有其他处理器要修改处理器 0 中的 cache line 状态将会被阻塞。同时，假如此时处理器 1 要读取相应的 cache line 数据，则会发现状态为 I(Invalid)。于是处理器 1 向总线中发出 read 消息，处理器 0 嗅探到 read 消息后，将会从自己的高速缓存或者主内存中将数据发送到总线，并将自身对应的 cache line 状态置为 S(Shared)，处理器 1 从总线中接收到 read 消息后，将最新的数据写入到对应的 cache line，并将状态置为 S(Shared)。由此处理 0 与处理器 1 中对应的 cache line 状态又都变成了 S(Shared)。

更新和读取数据的过程如下所示：

![更新数据流程](https://s3.ax1x.com/2021/01/28/y9tjot.png)

![读取数据流程](https://s3.ax1x.com/2021/01/28/y9N9SS.png)

MESI 协议能保证各个处理器间的高速缓存数据一致性，但是同样带来两个严重的效率问题：

- 当处理器 0 向总线发送 invalidate 消息后，要等到所有其他拥有相同缓存的处理器返回 invalidate ack 消息才能将对应的 cache line 状态置为 E 并进行修改,但是在这过程中它一直是处于阻塞状态，这将严重影响处理器的性能
- 当处理 1 嗅探到 invalidate 消息后，会先去将对应的 cache line 状态置为 I，然后才会返回 invalidate ack 消息到总线，这个过程也是影响性能的。

基于以上两个问题，设计者又引入了写缓冲器和无效队列。在上面的场景中，处理器 0，先将要修改的数据放入写缓冲器，再向总线发出 invalidate 消息来通知其他有相同缓存的处理器缓存失效，处理器 0 就可以继续执行其他指令，当接收到其他所有处理器的 invalidate ack 后，再将处理器 0 中的 cache line 置为 E，并将写缓冲器中的数据写入高速缓存。处理器 1 从总线嗅探到 invalidate 消息后，先将消息放入到无效队列，接着立刻返回 invalidate ack 消息。这样来提高处理的速度，达到提高性能的目的。

写缓冲器和无效队列提高 MESI 协议下处理器性能，但同时也带来了新的可见性与有序性问题如下：

![MESI 带来的可见性与有序性问题](https://s3.ax1x.com/2021/01/28/y9NBmd.md.png)

如上图所示：假设最初共享变量 x=0 同时存在于处理 0 和处理 1 的高速缓存中，且对应状态为 S(Shared)，此时处理 0 要将 x 的值改变成 1，先将值写到写缓冲器里，然后向总线发送 invalidate 消息，同时处理器 1 希望将 x 的值加 1 赋给 y，此时处理器 1 发现自身缓存中 x=0 状态为 S，则直接用 x=0 进行参与计算，从而发生了错误，显然这个错误由写缓冲器和无效队列导致的，因为 x 的新值还在写缓冲器中，无效消息在处理 1 的无效队列中。

## 典型案例：并发加

可见性问题最经典的案例即是并发加操作，如下两个线程同时在更新变量 test 的 count 属性域的值，第一次都会将 `count=0` 读到各自的 CPU 缓存里，执行完 `count+=1` 之后，各自 CPU 缓存里的值都是 1，同时写入内存后，我们会发现内存中是 1，而不是我们期望的 2。之后由于各自的 CPU 缓存里都有了 count 的值，两个线程都是基于 CPU 缓存里的 count 值来计算，所以导致最终 count 的值都是小于 20000 的。

```java
Thread th1 = new Thread(()->{
    test.add10K();
});

Thread th2 = new Thread(()->{
    test.add10K();
});

// 每个线程中对相同对象执行加操作
count += 1;
```

在 Java 中，如果多个线程共享一个对象，并且没有合理的使用 volatile 声明和线程同步，一个线程更新共享对象后，另一个线程可能无法取到对象的最新值。当一个共享变量被 volatile 修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。通过 synchronized 和 Lock 也能够保证可见性，synchronized 和 Lock 能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。

# 内存屏障

编译器优化乱序和 CPU 执行乱序的问题可以分别使用优化屏障（Optimization Barrier）和内存屏障（Memory Barrier）这两个机制来解决。多处理器同时访问共享主存，每个处理器都要对读写进行重新排序，一旦数据更新，就需要同步更新到主存上 。在这种情况下，代码和指令重排，再加上缓存延迟指令结果输出导致共享变量被修改的顺序发生了变化，使得程序的行为变得无法预测。为了解决这种不可预测的行为，处理器提供一组机器指令来确保指令的顺序要求，它告诉处理器在继续执行前提交所有尚未处理的载入和存储指令。同样的也可以要求编译器不要对给定点以及周围指令序列进行重排。这些确保顺序的指令称为内存屏障，简单的说内存屏障做了两件事情：**拒绝重排，更新缓存**。

- 优化屏障 (Optimization Barrier)：避免编译器的重排序优化操作，保证编译程序时在优化屏障之前的指令不会在优化屏障之后执行；这就保证了编译时期的优化不会影响到实际代码逻辑顺序。

- 内存屏障 (Memory Barrier)分为写屏障（Store Barrier）、读屏障（Load Barrier）和全屏障（Full Barrier），其作用有两个：防止指令之间的重排序、保证数据的可见性。
  - 写屏障：强制将写缓冲器中的内容写入到高速缓存中，或者将屏障之后的指令全部写到写缓冲器直到之前写缓冲器中的内容全部被刷回缓存中，也就是处理 0 必须等到所有的 invalidate ack 消息后，才能执行后续的操作，相当于 flush 操作；
  - 读屏障：处理器在读取数据前，必须强制检查无效队列中是否有 invalidate 消息，如果有必须先处理完无效队列汇总的无效消息，再进行数据读取,相当于 refresh 操作。

通过加入读写屏障保证了可见性与有序性。之所以说保证了有序性，是因为指令乱序现象就是写缓冲器异步接收到其他处理器中的 invalidate ack 消息后，再执行写缓冲器中的内容，导致本应该执行的指令顺序发生错乱。通过加入写屏障后保证了异步操作之后才能执行后续的指令，保证了原来的指令顺序。POSIX、C++、Java 都有各自的共享内存模型，实现上并没有什么差异，只是在一些细节上稍有不同。这里所说的内存模型并非是指内存布 局，特指内存、Cache、CPU、写缓冲区、寄存器以及其他的硬件和编译器优化的交互时对读写指令操作提供保护手段以确保读写序。将这些繁杂因素可以笼统的归纳为两个方面：重排和缓存，即上文所说的代码重排、指令重排和 CPU Cache。

内存屏障的意义重大，是确保正确并发的关键。通过正确的设置内存屏障可以确保指令按照我们期望的顺序执行。这里需要注意的是内存屏蔽只应该作用于需要同步的指令或者还可以包含周围指令的片段。如果用来同步所有指令，目前绝大多数处理器架构的设计就会毫无意义。

## Lock 指令

以 Java 中的 volatile 指令为例，它有 volatile 变量修饰的共享变量进行写操作的时候会多出第二行汇编代码：`lock addl $0×0,(%esp);`，通过查 IA-32 架构软件开发者手册可知，Lock 前缀的指令在多核处理器下会引发了两件事情：

- Lock 前缀指令会引起处理器缓存回写到内存。Lock 前缀指令导致在执行指令期间，声言处理器的 LOCK#信号。在多处理器环境中，LOCK#信号确保在声言该信号期间，处理器可以独占任何共享内存。但是，在最近的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕竟锁总线开销的比较大。对于 Intel486 和 Pentium 处理器，在锁操作时，总是在总线上声言 LOCK#信号。但在 P6 和目前的处理器中，如果访问的内存区域已经缓存在处理器内部，则不会声言 LOCK#信号。相反，它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为“缓存锁定”，缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据。

- 一个处理器的缓存回写到内存会导致其他处理器的缓存无效。IA-32 处理器和 Intel 64 处理器使用 MESI（修改、独占、共享、无效）控制协议去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32 和 Intel 64 处理器能嗅探其他处理器访问系统内存和它们的内部缓存。处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。例如，在 Pentium 和 P6 family 处理器中，如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处于共享状态，那么正在嗅探的处理器将使它的缓存行无效，在下次访问相同内存地址时，强制执行缓存行填充。

C++11 提供一组用户 API std::memory_order 来指导处理器读写顺序。Java 使用 happens-before 规则来屏蔽具体细节保证，指导 JVM 在指令生成的过程中穿插屏障指令。内存屏障也可以在编译期间指示对指令或者包括周围指令序列不进行优化，称之为编译器屏障，相当于轻量级内存屏障，它的工作同样重要，因为它在编译期指导编译器优化。屏障的实现稍微复杂一些，我们使用一组抽象的假想指令来描述内存屏障的工作原理。使用 MB_R、MB_W、MB 来抽象处理器指令为宏：

- MB_R 代表读内存屏障，它保证读取操作不会重排到该指令调用之后。
- MB_W 代表写内存屏障，它保证写入操作不会重排到该指令调用之后。
- MB 代表读写内存屏障，可保证之前的指令不会重排到该指令调用之后。

这些屏障指令在单核处理器上同样有效，因为单处理器虽不涉及多处理器间数据同步问题，但指令重排和缓存仍然影响数据的正确同步。指令重排是非常底层的且实 现效果差异非常大，尤其是不同体系架构对内存屏障的支持程度，甚至在不支持指令重排的体系架构中根本不必使用屏障指令。具体如何使用这些屏障指令是支持的 平台、编译器或虚拟机要实现的，我们只需要使用这些实现的 API(指的是各种并发关键字、锁、以及重入性等，下节详细介绍)。这里的目的只是为了帮助更好 的理解内存屏障的工作原理。

# Cache Line & False Sharing | 缓存行与伪共享

缓存系统中是以缓存行（Cache Line）为单位存储的，缓存行是 2 的整数幂个连续字节，一般为 32-256 个字节。最常见的缓存行大小是 64 个字节。当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。

[![image.png](https://i.postimg.cc/prfFbyqz/image.png)](https://postimg.cc/JHhnj7wz)

若两个变量放在同一个缓存行中，在多线程情况下，可能会相互影响彼此的性能。如上图所示，CPU1 上的线程更新了变量 X，则 CPU 上的缓存行会失效，同一行的 Y 即使没有更新也会失效，导致 Cache 无法命中。同样地，若 CPU2 上的线程更新了 Y，则导致 CPU1 上的缓存行又失效。如果 CPU 经常不能命中缓存，则系统的吞吐量则会下降。这就是伪共享问题。

解决伪共享问题，可以在变量的前后都占据一定的填充位置，尽量让变量占用一个完整的缓存行。如上图中，CPU1 上的线程更新了 X，则 CPU2 上的 Y 则不会失效。同样地，CPU2 上的线程更新了 Y，则 CPU1 的不会失效。参考 Java 内存布局可知，所有对象都有两个字长的对象头。第一个字是由 24 位哈希码和 8 位标志位(如锁的状态或作为锁对象)组成的 Mark Word。第二个字是对象所属类的引用。如果是数组对象还需要一个额外的字来存储数组的长度。每个对象的起始地址都对齐于 8 字节以提高性能。因此当封装对象的时候为了高效率，对象字段声明的顺序会被重排序成下列基于字节大小的顺序：

```sh
doubles (8) 和 longs (8)
ints (4) 和 floats (4)
shorts (2) 和 chars (2)
booleans (1) 和 bytes (1)
references (4/8)
<子类字段重复上述顺序>
```

一条缓存行有 64 字节, 而 Java 程序的对象头固定占 8 字节(32 位系统)或 12 字节(64 位系统默认开启压缩, 不开压缩为 16 字节)。我们只需要填 6 个无用的长整型补上 `6*8=48` 字节，让不同的 VolatileLong 对象处于不同的缓存行, 就可以避免伪共享了；64 位系统超过缓存行的 64 字节也无所谓，只要保证不同线程不要操作同一缓存行就可以。这个办法叫做补齐（Padding）：

```java
public final static class VolatileLong
{
    public volatile long value = 0L;
    public long p1, p2, p3, p4, p5, p6; // 添加该行，错开缓存行，避免伪共享
}
```

某些 Java 编译器会将没有使用到的补齐数据, 即示例代码中的 6 个长整型在编译时优化掉, 可以在程序中加入一些代码防止被编译优化。

```java
public static long preventFromOptimization(VolatileLong v) {
	return v.p1 + v.p2 + v.p3 + v.p4 + v.p5 + v.p6;
}
```

# Links

- 并发吹剑录（一）：CPU 缓存一致性协议 MESI https://cubox.pro/c/QsiEBF
